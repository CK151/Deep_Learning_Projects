# -*- coding: utf-8 -*-
"""Intro_NN_Boston.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YC0aNi1IthOBXPS6FVjeEBjK42_D5dYt

# Boston House Prediction
"""

# importing libraries
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt


from sklearn.preprocessing import MinMaxScaler # preprocessing

"""### Load the dataset from Tensorflow"""

# i will use tuple to access split the dataset directly from the keras library

(train_data,train_targets), (test_data, test_targets) = tf.keras.datasets.boston_housing.load_data(
    path='boston_housing.npz', test_split=0.2, seed=42
)

# checking the shape of the train data
train_data.shape, train_targets.shape

# checking the shape of the test data
test_data.shape, test_targets.shape

train_data

test_data

"""*train_data has 13 features and 404 rows*

### Normalising the dataset
Since Neural Netwoek prefer normalised data i will normalise the data for better performance using the MinMaxScaler()
"""

# instantiating the MinMaxScaler
scaler = MinMaxScaler()

# fitting only on the train data to avoid information leakage
scaler.fit_transform(train_data)

# Transformimng on both the the train and test daata
train_data_scaled = scaler.transform(train_data)
test_data_scaled = scaler.transform(test_data)

train_data_scaled

test_data_scaled

"""# Building a Neural Network model

#### Model 1
"""

# setting random seed
tf.random.set_seed(42)

# building a very simple model and will improve as we go
model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])


# Compile the model

model_1.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["mae"])

# learning rate callback
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epochs: 1e-4 * 10 **(epochs/20))


# fit the model

history_1 = model_1 .fit(train_data_scaled,
             train_targets,
             epochs=35,
             callbacks=[lr_scheduler],
             validation_split=0.1) # using 10% of the train data for validation to ensure improvement

# checkout the history

pd.DataFrame(history_1.history).plot()
plt.title("Loss curve model 1")

"""As we can visualize, the model is too basic to learn any pattern from the data"""

model_1.evaluate(test_data_scaled, test_targets)

"""We used the Mean Absolute Error as our metric and the result shows that on the average we deviate by 20 from the original value

#### Model 2
"""

# set random seed
tf.random.set_seed(42)

# build the model
model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation="relu"), # added an additional layer with 100 neurons with relu activation function
    tf.keras.layers.Dense(1)
])


# Compile the model

model_2.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["mae"])
# learning rate callback
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epochs: 1e-4 * 10 **(epochs/20))

# fit the 2odel

history_2 = model_2 .fit(train_data_scaled,
                         train_targets,
                         epochs=30,
                         verbose=1,
                         callbacks=[lr_scheduler],
                         validation_split=0.1)

model_2.evaluate(test_data_scaled, test_targets)

# checkout the history

pd.DataFrame(history_2.history).plot()
plt.title("Loss curve model 1")

# plot the learning rate vs the loss

lrs = 1e-4 * (10 **(tf.range(30)/20))
plt.figure(figsize=(10,7))
plt.semilogx(lrs,history_2.history["loss"])
plt.xlabel("Learning Rate")
plt.ylabel("Loss")
plt.title("Learning rate vs Loss")

"""#### Model 3"""

tf.random.set_seed(42)

model_3 = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(1)
])


# Compile the model

model_3.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), # set the learning rate to 0.0002
                metrics=["mae"])


# fit the 2odel

history_3 = model_3.fit(train_data_scaled, train_targets, epochs=100, verbose=0)

# model evaluation
model_3.evaluate(test_data_scaled, test_targets)

pd.DataFrame(history_3.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs");

"""#### Model 4"""

# set random seed
tf.random.set_seed(42)

# build the model
model_4 = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation="relu"), # added 2 extra layer and used relu activation function
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(1, activation="linear") # used the linear activation function at the output layer
])


# Compile the model

model_4.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # set the learning rate to 0.01
                metrics=["mae"])


# fit the Model

history_4 = model_4.fit(train_data_scaled, train_targets, epochs=20, verbose=0)

model_4.evaluate(test_data_scaled, test_targets)

pd.DataFrame(history_4.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs");

predictions = model_4.predict(test_data_scaled)

test_targets.shape, predictions.shape

# squeezing the pred value so as to get it to same shape with the test_target

predictions = np.squeeze(predictions)

# putting the pred to a pandas dataframe
predictions_df = pd.DataFrame({'Prediction': predictions, 'Actual': test_targets})

# Display the first few rows of the DataFrame
print(predictions_df)

# adding the Residual column which is the difference between the predicted and actual values
predictions_df["Residual"] = predictions_df["Prediction"] - predictions_df["Actual"]

predictions_df.head()

# adding a differnce column that we tell the difference in percentage

predictions_df["Difference %"] = np.absolute(predictions_df["Residual"] / predictions_df["Actual"] * 100)

predictions_df

pd.set_option('display.float_format', lambda x: '%.2f' % x)
predictions_df.sort_values(by=['Difference %']) # sorting

predictions_df.describe()

"""With the pandas describe method we can see that the prediction has a mean of 20.4 with an standard devaition of 7.23 and the Actual value (test_targets) has a mean of 22 with std of 8 , Model 4 was choosen because it performes best"""

