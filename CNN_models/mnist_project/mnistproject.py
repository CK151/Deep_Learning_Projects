# -*- coding: utf-8 -*-
"""mnistProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xqawIngS3t8__8yrmZboeJS1qskJpYd5

# MNIST PROJECT

### Import Libraries
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

"""### Load the data"""

mnist_dataset, mnist_info = tfds.load(name="mnist", with_info=True, as_supervised=True)

mnist_dataset

"""Here we can see that the "mnist_dataset" contains the test and train data, with a shape of (28,28, 1) that shows that they are all tensor and has a grayscale image."""

# creating the train and test sets from the mnist dataset
mnist_train, mnist_test = mnist_dataset["train"],mnist_dataset["test"]

"""### Visualize the data in image mode"""

# Shuffle the training dataset and extract random samples
num_samples = 10  # Number of random samples to display
shuffled_mnist_train = mnist_train.shuffle(buffer_size=10000)

# Take random samples from the shuffled dataset
random_samples = shuffled_mnist_train.take(num_samples)

# Plot the random samples
plt.figure(figsize=(10, 10))
for i, (image, label) in enumerate(random_samples):
    plt.subplot(5, 5, i + 1)  # Create subplots in a 5x5 grid
    plt.imshow(image.numpy(), cmap='gray')
    plt.title(f'Label: {label.numpy()}')
    plt.axis('off')

plt.tight_layout()
plt.show();

"""### Preprocessing"""

# setting the number of samples to 10% of the dataset
num_val_samples = 0.1 * mnist_info.splits["train"].num_examples

# to make sure that there is an Integer value we cast it to integer value using tf.cast
num_val_samples = tf.cast(num_val_samples, tf.int64)
print(f"The number of validation samples are : {num_val_samples.numpy()}")

# getting the number of test samples from the test data
num_test_samples = mnist_info.splits["test"].num_examples


#casting it to an integer value
num_test_samples = tf.cast(num_test_samples, tf.int64)

print(f"The number of test samples are: {num_test_samples.numpy()}")

# defing a function that will scaled the data which is normalising the data to values between 0 & 1
def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255.
  return image, label

# using the map method to apply the scale function we created
scaled_train_and_val_data = mnist_train.map(scale)

test_data = mnist_test.map(scale)

# shuffling the data
BUFFER_SIZE = 10000

shuffled_train_and_val_data = scaled_train_and_val_data.shuffle(BUFFER_SIZE)


val_data = shuffled_train_and_val_data.take(num_val_samples)
train_data = shuffled_train_and_val_data.skip(num_val_samples)

#Batching

BATCH_SIZE = 32


train_data = train_data.batch(BATCH_SIZE)

#the idea here is to batch the entire val data in a single iteration
val_data = val_data.batch(num_val_samples)

#the idea here is to batch the entire test data in a single iteration
test_data = test_data.batch(num_test_samples)

val_inputs, val_targets = next(iter(val_data))

val_targets.numpy()

val_inputs.shape

"""### Model 1"""

INPUT_SIZE = 784 #28 by 28  if converted to a vector
OUTPUT_SIZE = 10 # we have 10 classes 0 -9
HIDDEN_LAYER_SIZE = 100

model_1 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape= (28,28,1)), # converts it to a vector
    tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation="relu"),
    tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation="relu"),
    tf.keras.layers.Dense(OUTPUT_SIZE, activation="softmax") # using softmax because it is a multiclass classification
])

"""### Choosing the Optimizer and the loss function"""

model_1.compile(optimizer="adam",
                loss="sparse_categorical_crossentropy",  # because the classes are not one_hot_encoded
                metrics=["accuracy"])

"""### Training"""

NUM_EPOCHS = 10
history_1 = model_1.fit(train_data,
                        epochs=NUM_EPOCHS,
                        validation_data=(val_inputs, val_targets),
                        verbose=2)

"""### Test the Model"""

test_loss, test_accuracy = model_1.evaluate(test_data)

print("Test loss: {0:.2f}. Test accuracy {1:.2f}%".format(test_loss, test_accuracy*100))

# to visualize the loss and accuracy curve
def plot_loss_curves(history):
  """
  Returns seprate loss curves for training abd validation metrics
  """

  loss = history.history["loss"]
  val_loss = history.history["val_loss"]

  epochs = range(len(history.history["loss"]))

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  # plot loss

  plt.plot(epochs, loss, label="Training loss")
  plt.plot(epochs, val_loss, label="Val_loss")
  plt.title("loss")
  plt.xlabel("epochs")
  plt.legend()

  # plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label="Training Accuracy")
  plt.plot(epochs, val_accuracy, label="Validation Accuracy")
  plt.title("accuracy")
  plt.xlabel("epochs")
  plt.legend()

plot_loss_curves(history_1)

"""After evaluating on the test data we are not allowed to change the model, this is because the test data is no longer new to the model

## Model 2

Here we will try and change the archetecture
"""

# Create the model (same as model_5 and model_6)
model_2 = tf.keras.Sequential([
  tf.keras.layers.Conv2D(filters=10, kernel_size=3, input_shape=(28, 28, 1), activation='relu'),
  tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu'),
  tf.keras.layers.MaxPool2D(),
  tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu'),
  tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu'),
  tf.keras.layers.MaxPool2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(OUTPUT_SIZE, activation='softmax')
])

# Compile the model
model_2.compile(loss="sparse_categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

# Fit the model
NUM_EPOCHS = 5
history_2 = model_2.fit(train_data,
                        epochs=NUM_EPOCHS,
                        validation_data=(val_inputs, val_targets),
                        verbose=2)

test_loss_2, test_accuracy_2 = model_2.evaluate(test_data)

print("Test loss: {0:.2f}. Test accuracy {1:.2f}%".format(test_loss_2, test_accuracy_2*100))

plot_loss_curves(history=history_2)

"""Model 2 out performes model 1 and it shows that the model is not overfitting because there is a steady decrease in the val_loss and a steady increase in val accuracy

**Now i will fit for longer epoch and add more filters** which will be model 3

### Model 3
"""

model_3 = tf.keras.Sequential([
  tf.keras.layers.Conv2D(filters=HIDDEN_LAYER_SIZE, kernel_size=3, input_shape=(28, 28, 1), activation='relu'),
  tf.keras.layers.MaxPool2D(),
  tf.keras.layers.Conv2D(filters=HIDDEN_LAYER_SIZE, kernel_size=3, activation='relu'),
  tf.keras.layers.MaxPool2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(OUTPUT_SIZE, activation='softmax')
])

# Compile the model
model_3.compile(loss="sparse_categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

# Fit the model
NUM_EPOCHS = 5
history_3 = model_3.fit(train_data,
                        epochs=NUM_EPOCHS,
                        validation_data=(val_inputs, val_targets),
                        verbose=2)

test_loss_3, test_accurAcy_3 = model_3.evaluate(test_data)

print("Test loss: {0:.2f}. Test accuracy {1:.2f}%".format(test_loss_3, test_accurAcy_3*100))

plot_loss_curves(history=history_3)

model_3.summary()

model_2.summary()

"""# Summary

We can see that the model 3 architecture is more simple that model 2 and still hits 98%  however we know model 1 is simple feedforward neural network but works well too

## Plot Confusion matrix to visualize the outcomes
"""

# create confusion matrix
import itertools
from sklearn.metrics import confusion_matrix
figsize= (10,10)

def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10,10), text_size=15):
  # create the confusion matrix

  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])

  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes),
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)

  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
             horizontalalignment="center",
             color="white" if cm[i, j] > threshold else "black",
             size=text_size)

test_inputs, test_targets = next(iter(test_data))

test_targets

y_probs = model_3.predict(test_inputs)

y_probs

y_probs[0], tf.argmax(y_probs[0]), test_targets[tf.argmax(y_probs[0])]

y_preds = y_probs.argmax(axis=1)

y_preds

confusion_matrix(y_true=test_targets,
                 y_pred=y_preds)

# make a better confusion matrix

make_confusion_matrix(y_true=test_targets,
                      y_pred= y_preds,
                      figsize=(15,15),
                      text_size=10)

"""Here we can see that our model is confusing 7 and 2  which is understandable it has the highest rate of 26% followed by 7 and 9 which has 12%. What we can do is get a better data"""

#lets visualize the test data


# Shuffle the training dataset and extract random samples
num_samples = 10  # Number of random samples to display
shuffled_mnist_test = mnist_test.shuffle(buffer_size=100)

# Take random samples from the shuffled dataset
random_samples = shuffled_mnist_test.take(num_samples)

# Plot the random samples
plt.figure(figsize=(10, 10))
for i, (image, label) in enumerate(random_samples):
    plt.subplot(5, 5, i + 1)  # Create subplots in a 5x5 grid
    plt.imshow(image.numpy(), cmap='gray')
    plt.title(f'Label: {label.numpy()}')
    plt.axis('off')

plt.tight_layout()
plt.show();

"""We can see why the model misunderstands 2 and 7 alot, is because some of the images are not clear enoughðŸ˜‡

I hope the notebook was easy to undersand please if you have any question reach out me on

- Email: andersonoki6@gmail.com
- X: @Chandoki15
"""

